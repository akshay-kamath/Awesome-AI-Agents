{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Webpage content:\n",
      "\n",
      " LLM Agents | Prompt Engineering Guide Prompt Engineering Guide🎓 Prompt Engineering Course🎓 Prompt Engineering CourseServicesServicesAboutAboutGitHubGitHub (opens in a new tab)DiscordDiscord (opens in a new tab)Prompt EngineeringIntroductionLLM SettingsBasics of PromptingPrompt ElementsGeneral Tips for Designing PromptsExamples of PromptsPrompting TechniquesZero-shot PromptingFew-shot PromptingChain-of-Thought PromptingMeta PromptingSelf-ConsistencyGenerate Knowledge PromptingPrompt ChainingTree of ThoughtsRetrieval Augmented GenerationAutomatic Reasoning and Tool-useAutomatic Prompt EngineerActive-PromptDirectional Stimulus PromptingProgram-Aided Language ModelsReActReflexionMultimodal CoTGraph PromptingAgentsIntroduction to AgentsAgent ComponentsGuidesOptimizing PromptsApplicationsFine-tuning GPT-4oFunction CallingContext Caching with LLMsGenerating DataGenerating Synthetic Dataset for RAGTackling Generated Datasets DiversityGenerating CodeGraduate Job Classification Case StudyPrompt FunctionPrompt HubClassificationSentiment ClassificationFew-Shot Sentiment ClassificationCodingGenerate Code SnippetGenerate MySQL QueryDraw TiKZ DiagramCreativityRhymesInfinite PrimesInterdisciplinaryInventing New WordsEvaluationEvaluate Plato's DialogueInformation ExtractionExtract Model NamesImage GenerationDraw a Person Using AlphabetMathematicsEvaluating Composite FunctionsAdding Odd NumbersQuestion AnsweringClosed Domain Question AnsweringOpen Domain Question AnsweringScience Question AnsweringReasoningIndirect ReasoningPhysical ReasoningText SummarizationExplain A ConceptTruthfulnessHallucination IdentificationAdversarial PromptingPrompt InjectionPrompt LeakingJailbreakingModelsChatGPTClaude 3Code LlamaFlanGeminiGemini AdvancedGemini 1.5 ProGemmaGPT-4Grok-1LLaMALlama 3Mistral 7BMistral LargeMixtralMixtral 8x22BOLMoPhi-2SoraLLM CollectionRisks & MisusesAdversarial PromptingFactualityBiasesLLM Research FindingsLLM AgentsRAG for LLMsLLM ReasoningRAG FaithfulnessLLM In-Context RecallRAG Reduces HallucinationSynthetic DataThoughtSculptInfini-AttentionLM-Guided CoTTrustworthiness in LLMsLLM TokenizationWhat is Groq?PapersToolsNotebooksDatasetsAdditional ReadingsEnglishLightOn This PageLLM Agent FrameworkAgentPlanningPlanning Without FeedbackPlanning With FeedbackMemoryToolsLLM Agent ApplicationsNotable LLM-based AgentsLLM Agent ToolsLLM Agent EvaluationChallengesReferencesQuestion? Give us feedback → (opens in a new tab)Edit this pageLLM Research FindingsLLM AgentsLLM Agents\n",
      "\n",
      "LLM based agents, hereinafter also referred to as LLM agents for short, involve LLM applications that can execute complex tasks through the use of an architecture that combines LLMs with key modules like planning and memory. When building LLM agents, an LLM serves as the main controller or \"brain\" that controls a flow of operations needed to complete a task or user request. The LLM agent may require key modules such as planning, memory, and tool usage.\n",
      "To better motivate the usefulness of an LLM agent, let's say that we were interested in building a system that can help answer the following question:\n",
      "\n",
      "What's the average daily calorie intake for 2023 in the United States?\n",
      "\n",
      "The question above could potentially be answered using an LLM that already has the knowledge needed to answer the question directly. If the LLM doesn't have the relevant knowledge to answer the question, it's possible to use a simple RAG system where an LLM has access to health related information or reports. Now let's give the system a more complex question like the following:\n",
      "\n",
      "How has the trend in the average daily calorie intake among adults changed over the last decade in the United States, and what impact might this have on obesity rates? Additionally, can you provide a graphical representation of the trend in obesity rates over this period?\n",
      "\n",
      "To answer such a question, just using an LLM alone wouldn't be enough. You can combine the LLM with an external knowledge base to form a RAG system but this is still probably not enough to answer the complex query above. This is because the complex question above requires an LLM to break the task into subparts which can be addressed using tools and a flow of operations that leads to a desired final response. A possible solution is to build an LLM agent that has access to a search API, health-related publications, and public/private health database to provide relevant information related to calorie intake and obesity.\n",
      "In addition, the LLM will need access to a \"code interpreter\" tool that helps take relevant data to produce useful charts that help understand trends in obesity. These are the possible high-level components of the hypothetical LLM agent but there are still important considerations such as creating a plan to address the task and potential access to a memory module that helps the agent keep track of the state of the flow of operations, observations, and overall progress.\n",
      "🎓Learn more about LLM-based agents and advanced prompting methods in our new AI courses. Join now! (opens in a new tab)Use code PROMPTING20 to get an extra 20% off.\n",
      "LLM Agent Framework\n",
      "\n",
      "Generally speaking, an LLM agent framework can consist of the following core components:\n",
      "\n",
      "User Request - a user question or request\n",
      "Agent/Brain - the agent core acting as coordinator\n",
      "Planning - assists the agent in planning future actions\n",
      "Memory - manages the agent's past behaviors\n",
      "\n",
      "Agent\n",
      "A large language model (LLM) with general-purpose capabilities serves as the main brain, agent module, or coordinator of the system. This component will be activated using a prompt template that entails important details about how the agent will operate, and the tools it will have access to (along with tool details).\n",
      "While not mandatory, an agent can be profiled or be assigned a persona to define its role. This profiling information is typically written in the prompt which can include specific details like role details, personality, social information, and other demographic information. According to [Wang et al. 2023], the strategies to define an agent profile include handcrafting, LLM-generated or data-driven.\n",
      "Planning\n",
      "Planning Without Feedback\n",
      "The planning module helps to break down the necessary steps or subtasks the agent will solve individually to answer the user request. This step is important to enable the agent to reason better about the problem and reliably find a solution. The planning module will leverage an LLM to decompose a detailed plan which will include subtasks to help address the user question. Popular techniques for task decomposition include Chain of Thought (opens in a new tab) and Tree of Thoughts (opens in a new tab) which can be categorized as single-path reasoning and multi-path reasoning, respectively. Below is a figure comparing different strategies as formalized in Wang et al., 2023 (opens in a new tab):\n",
      "\n",
      "Planning With Feedback\n",
      "The planning modules above don't involve any feedback which makes it challenging to achieve long-horizon planning to solve complex tasks. To address this challenge, you can leverage a mechanism that enables the model to iteratively reflect and refine the execution plan based on past actions and observations. The goal is to correct and improve on past mistakes which helps to improve the quality of final results. This is particularly important in complex real-world environments and tasks where trial and error are key to completing tasks. Two popular methods for this reflection or critic mechanism include ReAct (opens in a new tab) and Reflexion (opens in a new tab).\n",
      "As an example, ReAct combines reasoning and acting aimed at enabling an LLM to solve complex tasks by interleaving between a series of steps (repeated N times): Thought, Action, and Observation. ReAct receives feedback from the environment in the form of observations. Other types of feedback can include human and model feedback. The figure below shows an example of ReAct and the different steps involved in performing question answering:\n",
      "\n",
      "Learn more about ReAct here:\n",
      "ReAct Prompting\n",
      "Memory\n",
      "The memory module helps to store the agent's internal logs including past thoughts, actions, and observations from the environment, including all interactions between agent and user. There are two main memory types that have been reported in the LLM agent literature:\n",
      "\n",
      "Short-term memory - includes context information about the agent's current situations; this is typically realized by in-context learning which means it is short and finite due to context window constraints.\n",
      "Long-term memory - includes the agent's past behaviors and thoughts that need to be retained and recalled over an extended period of time; this often leverages an external vector store accessible through fast and scalable retrieval to provide relevant information for the agent as needed.\n",
      "\n",
      "Hybrid memory integrates both short-term memory and long-term memory to improve an agent's ability for long-range reasoning and accumulation of experiences.\n",
      "There are also different memory formats to consider when building agents. Representative memory formats include natural language, embeddings, databases, and structured lists, among others. These can also be combined such as in Ghost in the Minecraft (GITM (opens in a new tab)) that utilizes a key-value structure where the keys are represented by natural language and values are represented by embedding vectors.\n",
      "Both the planning and memory modules allow the agent to operate in a dynamic environment and enable it to effectively recall past behaviors and plan future actions.\n",
      "Tools\n",
      "Tools correspond to a set of tool/s that enables the LLM agent to interact with external environments such as Wikipedia Search API, Code Interpreter, and Math Engine. Tools could also include databases, knowledge bases, and external models. When the agent interacts with external tools it executes tasks via workflows that assist the agent to obtain observations or necessary information to complete subtasks and satisfy the user request. In our initial health-related query, a code interpreter is an example of a tool that executes code and generates the necessary chart information requested by the user.\n",
      "Tools are leveraged in different ways by LLMs:\n",
      "\n",
      "MRKL (opens in a new tab) is a framework that combines LLMs with expert modules that are either LLMs or symbolic (calculator or weather API).\n",
      "Toolformer (opens in a new tab) fine-tune LLMs to use external tool APIs.\n",
      "Function Calling (opens in a new tab) - augments LLMs with tool use capability which involves defining a set of tool APIs and providing it to the model as part of a request.\n",
      "HuggingGPT (opens in a new tab) - an LLM-powered agent that leverages LLMs as a task planner to connect various existing AI models (based on descriptions) to solve AI tasks.\n",
      "\n",
      "\n",
      "LLM Agent Applications\n",
      "\n",
      "The ChemCrow agent designed to complete tasks across organic synthesis, drug discovery, and materials design. Figure source: Bran et al., 2023\n",
      "In this section, we highlight examples of domains and case studies where LLM-based agents have been effectively applied due to their complex reasoning and common sense understanding capabilities.\n",
      "Notable LLM-based Agents\n",
      "\n",
      "Ma et al. (2023) (opens in a new tab) analyze the effectiveness of conversational agents for mental well-being support and find that the agent can help users cope with anxieties but it can sometimes produce harmful content.\n",
      "Horton (2023) (opens in a new tab) gives LLM-based agents endowment, preferences, and personalities to explore human economic behaviors in simulated scenarios.\n",
      "Generative Agents (opens in a new tab) and AgentSims (opens in a new tab) both aim to simulate human daily life in a virtual town by constructing multiple agents.\n",
      "Blind Judgement (opens in a new tab) employs several language models to simulate the decision-making processes of multiple judges; predicts the decisions of the real-world Supreme Court with better-than-random accuracy.\n",
      "Ziems et al. (2023) (opens in a new tab) presents agents that can assist researchers in tasks such as generating abstracts, scripting, and extracting keywords.\n",
      "ChemCrow (opens in a new tab) is an LLM chemistry agent that utilizes chemistry-related databases to autonomously plan and execute the syntheses of insect repellent, three organocatalysts, and guided discovery of a novel chromophore.\n",
      "[Boiko et al. (2023)] combines multiple LLMs for automating the design, planning, and execution of scientific experiments.\n",
      "Math Agents assist researchers in exploring, discovering, solving and proving mathematical problems. EduChat (opens in a new tab) and CodeHelp (opens in a new tab) are two other notable examples of LLM agents designed for education.\n",
      "Mehta et al. (2023) (opens in a new tab) propose an interactive framework that enables human architects to interact with AI agents to construct structures in a 3D simulation environment.\n",
      "ChatDev (opens in a new tab), ToolLLM (opens in a new tab), MetaGPT (opens in a new tab) are notable examples where AI agents show potential to automate coding, debugging, testing, and assist with other software engineering tasks.\n",
      "D-Bot (opens in a new tab) a LLM-based database administrator that continuously acquires database maintenance experience and provides diagnosis and optimization advice for databases.\n",
      "IELLM (opens in a new tab) applies LLMs to address challenges in the oil and gas industry.\n",
      "Dasgupta et al. 2023 (opens in a new tab) presents a unified agent system for embodied reasoning and task planning.\n",
      "OS-Copilot (opens in a new tab) a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications.\n",
      "\n",
      "LLM Agent Tools\n",
      "\n",
      "AutoGen capabilities; Figure Source: https://microsoft.github.io/autogen (opens in a new tab)\n",
      "Below are notable examples of tools and frameworks that are used to build LLM agents:\n",
      "\n",
      "LangChain (opens in a new tab): a framework for developing applications and agents powered by language models.\n",
      "AutoGPT (opens in a new tab): provides tools to build AI agents.\n",
      "Langroid (opens in a new tab): Simplifies building LLM applications with Multi-Agent Programming: agents as first-class citizens, collaborating on tasks via messages.\n",
      "AutoGen (opens in a new tab): a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks.\n",
      "OpenAgents (opens in a new tab): an open platform for using and hosting language agents in the wild.\n",
      "LlamaIndex (opens in a new tab) - a framework for connecting custom data sources to large language models.\n",
      "GPT Engineer (opens in a new tab): automate code generation to complete development tasks.\n",
      "DemoGPT (opens in a new tab): autonomous AI agent to create interactive Streamlit apps.\n",
      "GPT Researcher (opens in a new tab): an autonomous agent designed for comprehensive online research on a variety of tasks.\n",
      "AgentVerse (opens in a new tab): designed to facilitate the deployment of multiple LLM-based agents in various applications.\n",
      "Agents (opens in a new tab): an open-source library/framework for building autonomous language agents. The library supports features including long-short term memory, tool usage, web navigation, multi-agent communication, and brand new features including human-agent interaction and symbolic control.\n",
      "BMTools (opens in a new tab): extends language models using tools and serves as a platform for the community to build and share tools.\n",
      "crewAI (opens in a new tab): AI agent framework reimagined for engineers, offering powerful capabilities with simplicity to build agents and automations.\n",
      "Phidata (opens in a new tab): a toolkit for building AI Assistants using function calling.\n",
      "\n",
      "LLM Agent Evaluation\n",
      "\n",
      "AgentBench benchmark to evaluate LLM-as-Agent on real-world challenges and 8 different environments. Figure source: Liu et al. 2023\n",
      "Similar to evaluating LLM themselves, evaluating LLM agents is a challenging task. According to Wang et al., (2023), common evaluation methods include:\n",
      "\n",
      "Human Annotation: Includes human evaluators that directly score LLM results across different aspects that matter in the application such as honesty, helpfulness, engagement, unbiasedness, and more.\n",
      "Turing Test: Human evaluators are asked to compare results from real humans and agents where indistinguishable results mean that agents can achieve human-like performance.\n",
      "Metrics: These are carefully designed metrics that reflect the quality of the agents. Notable metrics include task success metrics, human similarity metrics, and efficiency metrics.\n",
      "Protocols: Corresponds to common evaluation protocols that determine how the metrics are used. Examples include real-world simulation, social evaluation, multi-task evaluation, and software testing.\n",
      "Benchmarks: Several benchmarks have been designed to evaluate LLM agents. Notable examples include ALFWorld (opens in a new tab), IGLU (opens in a new tab), Tachikuma (opens in a new tab), AgentBench (opens in a new tab), SocKET (opens in a new tab), AgentSims (opens in a new tab), ToolBench (opens in a new tab), WebShop (opens in a new tab), Mobile-Env (opens in a new tab), WebArena (opens in a new tab), GentBench (opens in a new tab), RocoBench (opens in a new tab), EmotionBench (opens in a new tab), PEB (opens in a new tab), ClemBench (opens in a new tab), and E2E (opens in a new tab).\n",
      "\n",
      "Challenges\n",
      "LLM agents are still in their infancy so there are many challenges and limitations that remain when building them:\n",
      "\n",
      "Role-playing capability: LLM-based agents typically need to adapt a role to effectively complete tasks in a domain. For roles that the LLM doesn't characterize well, it's possible to fine-tune the LLM on data that represent uncommon roles or psychology characters.\n",
      "Long-term planning and finite context length: planning over a lengthy history remains a challenge that could lead to errors that the agent may not recover from. LLMs are also limited in context length they can support which could lead to constraints that limit the capabilities of the agent such as leveraging short-term memory.\n",
      "Generalized human alignment: it's also challenging to align agents with diverse human values which is also common with standard LLMs. A potential solution involves the potential to realign the LLM by designing advanced prompting strategies.\n",
      "Prompt robustness and reliability: an LLM agent can involve several prompts designed to power the different modules like memory and planning. It's common to encounter reliability issues in LLMs with even the slightest changes to prompts. LLM agents involve an entire prompt framework which makes it more prone to robustness issues. The potential solutions include crafting prompt elements through trial and error, automatically optimizing/tuning prompts, or automatically generating prompts using GPT. Another common issue with LLMs is hallucination which is also prevalent with LLM agents. These agents rely on natural language to interface with external components that could be introducing conflicting information leading to hallucination and factuality issues.\n",
      "Knowledge boundary: similar to knowledge mismatch issues that could lead to hallucination or factuality issues, it's challenging to control the knowledge scope of LLMs which can significantly impact the effectiveness of simulations. Concretely, an LLM's internal knowledge could introduce biases or utilize user-unknown knowledge that could affect the agent's behavior when operating in specific environments.\n",
      "Efficiency: LLM agents involve a significant amount of requests that are handled by the LLM which could affect the efficiency of agent actions because it would depend heavily on the LLM inference speed. Cost is also a concern when deploying multiple agents.\n",
      "\n",
      "References\n",
      "\n",
      "LLM Powered Autonomous Agents (opens in a new tab)\n",
      "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning (opens in a new tab)\n",
      "A Survey on Large Language Model based Autonomous Agents (opens in a new tab)\n",
      "The Rise and Potential of Large Language Model Based Agents: A Survey (opens in a new tab)\n",
      "Large Language Model based Multi-Agents: A Survey of Progress and Challenges (opens in a new tab)\n",
      "Cognitive Architectures for Language Agents (opens in a new tab)\n",
      "Introduction to LLM Agents (opens in a new tab)\n",
      "LangChain Agents (opens in a new tab)\n",
      "Building Your First LLM Agent Application (opens in a new tab)\n",
      "Building LLM applications for production (opens in a new tab)\n",
      "Awesome LLM agents (opens in a new tab)\n",
      "Awesome LLM-Powered Agent (opens in a new tab)\n",
      "Functions, Tools and Agents with LangChain (opens in a new tab)\n",
      "LLM Research FindingsRAG for LLMsEnglishLightCopyright © 2024 DAIR.AI\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Webpage to scrape\n",
    "url = \"https://www.promptingguide.ai/research/llm-agents\"\n",
    "\n",
    "# Headers to mimic a browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "print(response)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Extract the desired information from the HTML\n",
    "    # For example, to get the repository name\n",
    "    webpage_content = soup.get_text()\n",
    "    \n",
    "    # Print the repository name\n",
    "    print(\"Webpage content:\\n\\n\", webpage_content)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"webpage_content.md\", \"w\") as f:\n",
    "    f.write(webpage_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/assistants \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"webpage_content.md\", \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"AI Agents Expert\",\n",
    "    description=\"You are an expert at AI Agents and can answer questions about them.\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    model=\"gpt-4o\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/vector_stores \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/vector_stores/vs_EszGI031tZlCkAhq2PYgjdxs/file_batches \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/vector_stores/vs_EszGI031tZlCkAhq2PYgjdxs/file_batches/vsfb_bda2eb04db1c4b18a482adbd92fc976d \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/vector_stores/vs_EszGI031tZlCkAhq2PYgjdxs/file_batches/vsfb_bda2eb04db1c4b18a482adbd92fc976d \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "vsfb_bda2eb04db1c4b18a482adbd92fc976d\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.beta.vector_stores.create(name=\"AI Agents Research\")\n",
    "\n",
    "file_paths = [\"./webpage_content.md\"]\n",
    "\n",
    "file_streams = [open(file_path, \"rb\") for file_path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "print(file_batch.status)\n",
    "\n",
    "print(file_batch.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/assistants/asst_3T17byAbqA8Tq6NM1rcu6IQn \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Let's update the assistant with the new vector store\n",
    "\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally we could attach the files directly in the thread\n",
    "# This would create its own vector store if no other vector store is already attached\n",
    "# to the assistant\n",
    "\n",
    "\n",
    "# Upload the user provided file to OpenAI\n",
    "# message_file = client.files.create(\n",
    "# file=open(\"edgar/aapl-10k.pdf\", \"rb\"), purpose=\"assistants\"\n",
    "# )\n",
    "\n",
    "# # Create a thread and attach the file to the message\n",
    "# thread = client.beta.threads.create(\n",
    "# messages=[\n",
    "#   {\n",
    "#     \"role\": \"user\",\n",
    "#     \"content\": \"How many shares of AAPL were outstanding at the end of of October 2023?\",\n",
    "#     # Attach the new file to the message.\n",
    "#     \"attachments\": [\n",
    "#       { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "#     ],\n",
    "#   }\n",
    "# ]\n",
    "# )\n",
    "\n",
    "# # The thread now has a vector store with that file in its tool resources.\n",
    "# print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/threads \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread_RanfLa8BqW08RiJe9qpBeHIN\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the core components of an LLM Agent framework?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/runs/run_1TGDfrqBWqCuCNO5MPlNnOzg \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_1TGDfrqBWqCuCNO5MPlNnOzg', assistant_id='asst_3T17byAbqA8Tq6NM1rcu6IQn', cancelled_at=None, completed_at=1736764645, created_at=1736764634, expires_at=None, failed_at=None, incomplete_details=None, instructions=None, last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=1736764636, status='completed', thread_id='thread_RanfLa8BqW08RiJe9qpBeHIN', tool_choice='auto', tools=[FileSearchTool(type='file_search', file_search=FileSearch(max_num_results=None, ranking_options=FileSearchRankingOptions(score_threshold=0.0, ranker='default_2024_08_21')))], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=Usage(completion_tokens=274, prompt_tokens=9013, total_tokens=9287, prompt_token_details={'cached_tokens': 0}), temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.openai.com/v1/threads/thread_RanfLa8BqW08RiJe9qpBeHIN/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyncCursorPage[Message](data=[Message(id='msg_e9yfp7Ii5U0H1iIh0mIDql6f', assistant_id='asst_3T17byAbqA8Tq6NM1rcu6IQn', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[FileCitationAnnotation(end_index=630, file_citation=FileCitation(file_id='file-QjhqQcqarzc2zNfGEg3raf'), start_index=618, text='【4:0†source】', type='file_citation'), FileCitationAnnotation(end_index=1078, file_citation=FileCitation(file_id='file-QjhqQcqarzc2zNfGEg3raf'), start_index=1066, text='【4:0†source】', type='file_citation')], value=\"The core components of an LLM Agent framework typically include:\\n\\n1. **User Request**: The initial input or question posed by the user.\\n   \\n2. **Agent/Brain**: The central component that acts as the coordinator. It is the large language model (LLM) responsible for processing the user request and orchestrating the overall operations.\\n   \\n3. **Planning**: A module to assist the agent in planning future actions. It helps in decomposing tasks and creating a detailed plan of subtasks necessary to address the user request. Planning can be enhanced with feedback to iteratively refine the plan based on past performance【4:0†source】.\\n\\n4. **Memory**: This component manages the agent's past behaviors and interactions. It typically includes two types of memory:\\n   - **Short-term Memory**: Contextual information about the agent's current situation, often hindered by context window constraints.\\n   - **Long-term Memory**: Retention and recall of past behaviors and experiences over an extended period, usually leveraging external vector storage for efficient retrieval【4:0†source】.\"), type='text')], created_at=1736764640, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_1TGDfrqBWqCuCNO5MPlNnOzg', status=None, thread_id='thread_RanfLa8BqW08RiJe9qpBeHIN'), Message(id='msg_a27GM9Jbqqu0Atvf4RyVid06', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What are the core components of an LLM Agent framework?'), type='text')], created_at=1736764226, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_RanfLa8BqW08RiJe9qpBeHIN')], object='list', first_id='msg_e9yfp7Ii5U0H1iIh0mIDql6f', last_id='msg_a27GM9Jbqqu0Atvf4RyVid06', has_more=False)\n"
     ]
    }
   ],
   "source": [
    "# Let's create a run and check the output\n",
    "# First without streaming\n",
    "\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "print(run)\n",
    "\n",
    "if run.status == 'completed': \n",
    "    messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    "    )\n",
    "    print(messages)\n",
    "else:\n",
    "    print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The core components of an LLM Agent framework typically include:\\n\\n1. **User Request**: The initial input or question posed by the user.\\n   \\n2. **Agent/Brain**: The central component that acts as the coordinator. It is the large language model (LLM) responsible for processing the user request and orchestrating the overall operations.\\n   \\n3. **Planning**: A module to assist the agent in planning future actions. It helps in decomposing tasks and creating a detailed plan of subtasks necessary to address the user request. Planning can be enhanced with feedback to iteratively refine the plan based on past performance【4:0†source】.\\n\\n4. **Memory**: This component manages the agent's past behaviors and interactions. It typically includes two types of memory:\\n   - **Short-term Memory**: Contextual information about the agent's current situation, often hindered by context window constraints.\\n   - **Long-term Memory**: Retention and recall of past behaviors and experiences over an extended period, usually leveraging external vector storage for efficient retrieval【4:0†source】.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example where we include file citations\n",
    "\n",
    "# run = client.beta.threads.runs.create_and_poll(\n",
    "#   thread_id=thread.id, assistant_id=assistant.id\n",
    "# )\n",
    "\n",
    "# messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "# message_content = messages[0].content[0].text\n",
    "# annotations = message_content.annotations\n",
    "# citations = []\n",
    "# for index, annotation in enumerate(annotations):\n",
    "#   message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "#   if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "#       cited_file = client.files.retrieve(file_citation.file_id)\n",
    "#       citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "# print(message_content.value)\n",
    "# print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-assistants",
   "language": "python",
   "name": "gpt-assistants"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
